\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nips15submit_e, times}

\title{Midway Report: Semi-Supervised Learning of Pen-Based OCR.}
\author{Joshua Brakensiek, Jacob Imola, Sidhanth Mohanty}

\begin{document}

\maketitle

\newcommand{\Seq}{\operatorname{Seq}}
\section{Background}
Handwriting recognition is a classic machine learning problem. Generally, an image of a digit is inputted to an algorithm and classified. Instead of taking this approach, we used two data sets,  ``Pen-Based Recognition of Handwritten Digits Data Set'' \cite{Alpaydin:1998} and  ``UJI Pen Characters (Version 2) Data Set'' \cite{Llorens:2008} from the UCI Machine Learning Repository \cite{Lichman:2013}, which consist of pixel coordinates that the writers' pens took at certain time intervals.  Each data set has over 10,000 data points. The first data set consists entirely of drawings of the digits $0, \hdots, 9$, while the second data set uses a much richer variety of characters. We seek to explore if adding the extra information of time while sacrificing some detail from the shape of the digits can produce a viable semi-supervised model.  We implement our learning algorithms in Python 2.x, using on the NumPy, SciPy, and Scikit-learn libraries (and possibly other libraries as we see fit). The models we try are Transductive Support Vector Machines and graph-based kernel methods and regularization. Throughout this paper, let $X = \{x_1, x_2, \ldots, x_k\}$ be the labeled training data, $Y = \{y_1, y_2, \ldots, y_k\}$ be the corresponding labels, $X^* = \{x^*_1, x^*_2, \ldots, x^*_n$ be the unlabeled training data, and $Y^* = \{y^*_1, y^*_2, \ldots, y^*_n$ be variables representing the unlabeled training data's labels. Now, we will give a description of the models: \par
Recall that an SVM reduces to the following primal problem:
\[
\arg\min_{w, b, \zeta} \frac{1}{2}w^Tw+C\sum_{i=1}^k\zeta_i
\]
\begin{align*}
\textrm{subject to}\quad \forall_{i=1}^k&: y_i(wx_i+b)\geq 1-\zeta_i \\
\forall_{i=1}^k&: \zeta_i>0
\end{align*}
A TSVM's primal problem allows the unlabeled data to take on any label, and hence reduces to a similar problem \cite{Joachims:1999}:
\begin{equation}\label{eq:1}
\arg\min_{w, b, \zeta, \zeta^*, Y^*} \frac{1}{2}w^Tw+C\sum_{i=1}^k\zeta_i+C^*\sum_{i=1}^n\zeta^*_i
\end{equation}
\begin{align*}
\textrm{subject to}\quad \forall_{i=1}^k&: y_i(wx_i+b)\geq 1-\zeta_i \\
\forall_{i=1}^n&: y^*_i(wx^*_i+b)\geq 1-\zeta^*_i \\
\forall_{i=1}^k&: \zeta_i>0 \\
\forall_{i=1}^n&: \zeta^*_i>0
\end{align*}
Here, $C$ and $C^*$ are parameters that control the type of fit we get.
\section{Related Work}
(Probably could cite some of our Bibliography here)
\section{Methods}
One TSVM method we used is an SVM with local search. The user inputs $C$ and $C^*$ defined in (\ref{eq:1}) and an additional paremeter, $num_+$. First, the $Y^*$ are initialized to be 
\section{Experiments}
\section{Future Plans}

\bibliographystyle{alpha}
\bibliography{midway}

\end{document}
